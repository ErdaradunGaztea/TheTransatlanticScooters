{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33310167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e67cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"XDaaAXDXD\").getOrCreate()\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp(x):\n",
    "    return x[np.argmin(np.abs(x))]\n",
    "    \n",
    "def process_row_python(row):\n",
    "    line = lines.loc[idx[row.line, :, :]]\n",
    "    stops = line.groupby(['busstopId', 'busstopNr']).first().loc[:, ['lon', 'lat']]\n",
    "    stops = np.sqrt(((stops - np.array([row.lon, row.lat]))**2).sum(axis=1)).sort_values().loc[idx[:, :]].head(4).index\n",
    "    chosen = line.loc[idx[stops.get_level_values(0), stops.get_level_values(1), :]]\n",
    "    chosen = chosen.time - row.time\n",
    "    best_iter = chosen.abs().groupby('iter').min().sort_values().index[0]\n",
    "    best_stops = chosen.loc[idx[:, :, best_iter]].abs().groupby('busstopId').min().index\n",
    "    times = chosen.loc[idx[best_stops, :, best_iter]]\n",
    "    return -times.groupby('busstopId').apply(tmp(x)).mean()\n",
    "\n",
    "def process_row_spark(row):\n",
    "    line = table.join(row, on='line')\n",
    "    w = Window.partitionBy('busstopId', 'busstopNr').orderBy(col(\"time\"))\n",
    "    stops = line.withColumn(\"row\", row_number().over(w)).filter(col(\"row\") == 1).drop(\"row\")\n",
    "    stops = stops.withColumn('coords', array(stops.lon, stops.lat))\n",
    "    stops = stops.withColumn('dists', distance_udf(stops.coords, stops.coords2))\n",
    "    stops = stops.orderBy('dists').limit(4).select('busstopId', 'busstopNr')\n",
    "    chosen = line.join(stops, on=['busstopId', 'busstopNr'])\n",
    "    chosen = chosen.withColumn('time_diff', chosen.time - chosen.time2)\n",
    "    chosen = chosen.withColumn('time_diff_abs', abs(chosen.time_diff))\n",
    "    best_iter = chosen.groupby('iter').agg(min('time_diff_abs').alias('time_dist')).orderBy('time_dist').limit(1).select('iter')\n",
    "    result = chosen.join(best_iter, on='iter').orderBy('time_diff_abs').limit(2).agg((-mean('time_diff')).alias('aaa')).select('aaa')\n",
    "    result = result.aaa\n",
    "    return result\n",
    "\n",
    "distance_udf = udf(lambda x, row: float(distance.euclidean(x, row)), FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e5596",
   "metadata": {},
   "source": [
    "### Timetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187bcc12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"instance-tram-1:9092\") \\\n",
    "  .option(\"subscribe\", \"lines\") \\\n",
    "  .load()\n",
    "\n",
    "table = df.select(col(\"value\").cast(\"string\")) .alias(\"csv\").select(\"csv.*\")\n",
    "table = table.selectExpr(\"split(value,',')[0] as busstopId\" \\\n",
    "                 ,\"split(value,',')[1] as busstopNr\" \\\n",
    "                 ,\"split(value,',')[2] as line\" \\\n",
    "                 ,\"split(value,',')[3] as direction\" \\\n",
    "                 ,\"split(value,',')[4] as lon\" \\\n",
    "                 ,\"split(value,',')[5] as lat\" \\\n",
    "                         ,\"split(value,',')[6] as time\" \\\n",
    "                         ,\"split(value,',')[7] as iter\")\n",
    "\n",
    "table = table.withColumn('busstopNr', table.busstopNr.cast(IntegerType()))\\\n",
    ".withColumn('line', table.line.cast(IntegerType()))\\\n",
    ".withColumn('lon', table.lon.cast(FloatType()))\\\n",
    ".withColumn('lat', table.lat.cast(FloatType()))\\\n",
    ".withColumn('time', table.time.cast(TimestampType()))\\\n",
    ".withColumn('iter', table.iter.cast(IntegerType()))\n",
    "table = table.withColumn('time', 60*hour(table.time) + minute(table.time))\n",
    "table = table.withColumn('coords', array(table.lon, table.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6616e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----+--------------------+---------+---------+----+----+--------------------+\n",
      "|busstopId|busstopNr|line|           direction|      lon|      lat|time|iter|              coords|\n",
      "+---------+---------+----+--------------------+---------+---------+----+----+--------------------+\n",
      "|     R-03|        0|  15|             Wołoska|  52.1885|20.999907| 223|   0|[52.1885, 20.999907]|\n",
      "|     3240|        7|  15|         Samochodowa| 52.18876| 21.00332| 225|   0|[52.18876, 21.00332]|\n",
      "|     R-04|        0|   4|\"Zgrupowania AK \"...|52.299137|20.934156| 225|   0|[52.299137, 20.93...|\n",
      "|     3116|        3|  15|    Telewizja Polska|52.188824|21.007105| 226|   0|[52.188824, 21.00...|\n",
      "|     R-04|        0|   1|\"Zgrupowania AK \"...|52.299137|20.934156| 227|   0|[52.299137, 20.93...|\n",
      "|     3115|        3|  15|      Metro Wierzbno| 52.18887|  21.0114| 227|   0| [52.18887, 21.0114]|\n",
      "|     6061|        5|   1|          Marymoncka|52.299557|20.935863| 228|   0|[52.299557, 20.93...|\n",
      "|     6014|        3|   4|         Przy Agorze|52.297985|20.942766| 228|   0|[52.297985, 20.94...|\n",
      "|     3114|        3|  15|         Królikarnia| 52.18895|21.016396| 228|   0|[52.18895, 21.016...|\n",
      "|     6013|        3|   4|                UKSW|52.294888| 20.94566| 229|   0|[52.294888, 20.94...|\n",
      "|     6011|        3|   4|   Szpital Bielański| 52.29012|20.950191| 230|   0|[52.29012, 20.950...|\n",
      "|     3007|        5|  15|            Puławska|52.189762| 21.02384| 230|   0|[52.189762, 21.02...|\n",
      "|     6014|        3|   1|         Przy Agorze|52.297985|20.942766| 230|   0|[52.297985, 20.94...|\n",
      "|     6013|        3|   1|                UKSW|52.294888| 20.94566| 231|   0|[52.294888, 20.94...|\n",
      "|     3007|        4|  15|       Malczewskiego|52.190685|21.024548| 231|   0|[52.190685, 21.02...|\n",
      "|     6010|        3|   4|                 AWF| 52.28773|20.952522| 231|   0|[52.28773, 20.952...|\n",
      "|     6011|        3|   1|   Szpital Bielański| 52.29012|20.950191| 232|   0|[52.29012, 20.950...|\n",
      "|     3006|        6|  15|       Park Dreszera| 52.19437|21.024326| 232|   0|[52.19437, 21.024...|\n",
      "|     6008|        3|   4|      Podleśna-IMiGW| 52.28333|20.956453| 232|   0|[52.28333, 20.956...|\n",
      "|     3005|        6|  15|         Morskie Oko|52.197018|21.024158| 233|   0|[52.197018, 21.02...|\n",
      "+---------+---------+----+--------------------+---------+---------+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe4951",
   "metadata": {},
   "source": [
    "### Tram positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3ad55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"instance-tram-1:9092\") \\\n",
    "  .option(\"subscribe\", \"tram_positions_processed\") \\\n",
    "  .load()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Lines', StringType()),\n",
    "    StructField('Lon', FloatType()),\n",
    "    StructField('VehicleNumber', StringType()),\n",
    "    StructField('Time', TimestampType()),\n",
    "    StructField('Lat', FloatType()),\n",
    "    StructField('Brigade', StringType())\n",
    "])\n",
    "\n",
    "table2 = df2.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\").select(from_json('value', schema).alias('temp')).select('temp.*')\n",
    "table2 = table2.withColumnRenamed('Lines', 'line')\\\n",
    ".withColumnRenamed('Lon', 'lon2')\\\n",
    ".withColumnRenamed('Time', 'time2')\\\n",
    ".withColumnRenamed('Lat', 'lat2')\n",
    "\n",
    "table2 = table2.withColumn('time2', 60*hour(table2.time2) + minute(table2.time2))\n",
    "table2 = table2.withColumn('coords2', array(table2.lon2, table2.lat2))\n",
    "# table2 = table2.select('line', 'lon2', 'lat2', 'time2', 'coords2')\n",
    "table2 = table2.withColumn('line', table2.line.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e773a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc67afd",
   "metadata": {},
   "source": [
    "### Static example - Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b17bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pd.Series([15, 52.2, 21, 700], index=['line', 'lon', 'lat', 'time'])\n",
    "process_row_python(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdb44d",
   "metadata": {},
   "source": [
    "### Static example - PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fdf425e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [[15, 52.21901, 20.983505, 600]]\n",
    "columns = ['line', 'lon2', 'lat2', 'time2']\n",
    "row = spark.sparkContext.parallelize(data).toDF(columns)\n",
    "row = row.withColumn('coords2', array(row.lon2, row.lat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_row_spark(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72c506",
   "metadata": {},
   "source": [
    "### Attempts with stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54eba009",
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 = table.join(table2, on='line')\n",
    "table3 = table3.withColumn('dists', distance_udf(table3.coords, table3.coords2))\\\n",
    ".withColumn('time_diff', table3.time - table3.time2)\n",
    "table3 = table3.withColumn('time_diff_abs', abs(table3.time_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d33f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('line', 'Brigade').orderBy(col(\"dists\"), col('time_diff_abs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d6cc7d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21253/3803787371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brigade'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, 'time_diff_abs').limit(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'limit'"
     ]
    }
   ],
   "source": [
    "table3.withColumn(\"row\", row_number().over(w)).filter(col(\"row\") <= 4).drop(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eae38c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/16 18:08:42 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-46d63f72-64c6-418f-9a91-8562bfe59d07. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/16 18:08:42 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5865577400>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+---------+---------+---------+---+---+----+----+----+----+-----+-------+-----+\n",
      "|line|busstopId|busstopNr|direction|lon|lat|time|iter|lon2|lat2|time2|coords2|delay|\n",
      "+----+---------+---------+---------+---+---+----+----+----+----+-----+-------+-----+\n",
      "+----+---------+---------+---------+---+---+----+----+----+----+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table3.withColumn('delay', process_row(table3)).writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc64c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kafka = table\\\n",
    "#     .withColumn('value', to_json(struct(table[\"`dt`\"], table[\"`main.temp`\"], table[\"`main.pressure`\"], table[\"`main.humidity`\"], table[\"`visibility`\"],\n",
    "#                                        table[\"`wind.speed`\"], table[\"`clouds.all`\"], table[\"`rain.3h`\"], table[\"`snow.3h`\"], table[\"`pop`\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
